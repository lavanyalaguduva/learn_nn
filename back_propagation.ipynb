{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO13GiuBbkDrMi/GzYzOcq8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyalaguduva/learn_nn/blob/main/back_propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "<b>CODING BACKPROPAGATION FROM SCRATCH: ON A SINGLE NEURON</b>\n",
        "</div>"
      ],
      "metadata": {
        "id": "11DIM5d0MTll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnRsSW0EL8q2",
        "outputId": "1f6f7a87-cee0-421f-ba87-cc4c41db39a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Loss: 36.0\n",
            "Iteration 2, Loss: 33.872399999999985\n",
            "Iteration 3, Loss: 31.870541159999995\n",
            "Iteration 4, Loss: 29.98699217744401\n",
            "Iteration 5, Loss: 28.21476093975706\n",
            "Iteration 6, Loss: 26.54726856821742\n",
            "Iteration 7, Loss: 24.978324995835766\n",
            "Iteration 8, Loss: 23.502105988581878\n",
            "Iteration 9, Loss: 22.113131524656684\n",
            "Iteration 10, Loss: 20.80624545154949\n",
            "Iteration 11, Loss: 19.576596345362915\n",
            "Iteration 12, Loss: 18.419619501351963\n",
            "Iteration 13, Loss: 17.331019988822064\n",
            "Iteration 14, Loss: 16.306756707482677\n",
            "Iteration 15, Loss: 15.343027386070442\n",
            "Iteration 16, Loss: 14.43625446755368\n",
            "Iteration 17, Loss: 13.583071828521268\n",
            "Iteration 18, Loss: 12.780312283455652\n",
            "Iteration 19, Loss: 12.024995827503426\n",
            "Iteration 20, Loss: 11.314318574097976\n",
            "Iteration 21, Loss: 10.645642346368787\n",
            "Iteration 22, Loss: 10.016484883698395\n",
            "Iteration 23, Loss: 9.424510627071816\n",
            "Iteration 24, Loss: 8.867522049011871\n",
            "Iteration 25, Loss: 8.34345149591527\n",
            "Iteration 26, Loss: 7.850353512506679\n",
            "Iteration 27, Loss: 7.386397619917536\n",
            "Iteration 28, Loss: 6.949861520580408\n",
            "Iteration 29, Loss: 6.539124704714106\n",
            "Iteration 30, Loss: 6.152662434665503\n",
            "Iteration 31, Loss: 5.7890400847767705\n",
            "Iteration 32, Loss: 5.446907815766464\n",
            "Iteration 33, Loss: 5.124995563854671\n",
            "Iteration 34, Loss: 4.8221083260308575\n",
            "Iteration 35, Loss: 4.537121723962434\n",
            "Iteration 36, Loss: 4.268977830076255\n",
            "Iteration 37, Loss: 4.016681240318748\n",
            "Iteration 38, Loss: 3.7792953790159096\n",
            "Iteration 39, Loss: 3.5559390221160707\n",
            "Iteration 40, Loss: 3.345783025909011\n",
            "Iteration 41, Loss: 3.148047249077789\n",
            "Iteration 42, Loss: 2.9619976566572896\n",
            "Iteration 43, Loss: 2.786943595148845\n",
            "Iteration 44, Loss: 2.622235228675549\n",
            "Iteration 45, Loss: 2.4672611266608238\n",
            "Iteration 46, Loss: 2.321445994075166\n",
            "Iteration 47, Loss: 2.1842485358253256\n",
            "Iteration 48, Loss: 2.0551594473580463\n",
            "Iteration 49, Loss: 1.9336995240191863\n",
            "Iteration 50, Loss: 1.8194178821496518\n",
            "Iteration 51, Loss: 1.7118902853146067\n",
            "Iteration 52, Loss: 1.6107175694525138\n",
            "Iteration 53, Loss: 1.515524161097869\n",
            "Iteration 54, Loss: 1.4259566831769857\n",
            "Iteration 55, Loss: 1.3416826432012259\n",
            "Iteration 56, Loss: 1.2623891989880334\n",
            "Iteration 57, Loss: 1.18778199732784\n",
            "Iteration 58, Loss: 1.1175840812857634\n",
            "Iteration 59, Loss: 1.0515348620817766\n",
            "Iteration 60, Loss: 0.9893891517327431\n",
            "Iteration 61, Loss: 0.930916252865338\n",
            "Iteration 62, Loss: 0.8758991023209968\n",
            "Iteration 63, Loss: 0.8241334653738256\n",
            "Iteration 64, Loss: 0.7754271775702323\n",
            "Iteration 65, Loss: 0.7295994313758316\n",
            "Iteration 66, Loss: 0.6864801049815187\n",
            "Iteration 67, Loss: 0.6459091307771115\n",
            "Iteration 68, Loss: 0.6077359011481847\n",
            "Iteration 69, Loss: 0.571818709390327\n",
            "Iteration 70, Loss: 0.5380242236653578\n",
            "Iteration 71, Loss: 0.5062269920467349\n",
            "Iteration 72, Loss: 0.47630897681677353\n",
            "Iteration 73, Loss: 0.4481591162869011\n",
            "Iteration 74, Loss: 0.4216729125143454\n",
            "Iteration 75, Loss: 0.3967520433847474\n",
            "Iteration 76, Loss: 0.3733039976207088\n",
            "Iteration 77, Loss: 0.35124173136132436\n",
            "Iteration 78, Loss: 0.3304833450378702\n",
            "Iteration 79, Loss: 0.3109517793461322\n",
            "Iteration 80, Loss: 0.29257452918677535\n",
            "Iteration 81, Loss: 0.27528337451183676\n",
            "Iteration 82, Loss: 0.25901412707818716\n",
            "Iteration 83, Loss: 0.24370639216786655\n",
            "Iteration 84, Loss: 0.22930334439074554\n",
            "Iteration 85, Loss: 0.21575151673725296\n",
            "Iteration 86, Loss: 0.2030006020980815\n",
            "Iteration 87, Loss: 0.1910032665140846\n",
            "Iteration 88, Loss: 0.17971497346310225\n",
            "Iteration 89, Loss: 0.16909381853143338\n",
            "Iteration 90, Loss: 0.1591003738562249\n",
            "Iteration 91, Loss: 0.14969754176132236\n",
            "Iteration 92, Loss: 0.14085041704322837\n",
            "Iteration 93, Loss: 0.13252615739597357\n",
            "Iteration 94, Loss: 0.12469386149387143\n",
            "Iteration 95, Loss: 0.11732445427958357\n",
            "Iteration 96, Loss: 0.1103905790316602\n",
            "Iteration 97, Loss: 0.1038664958108892\n",
            "Iteration 98, Loss: 0.09772798590846558\n",
            "Iteration 99, Loss: 0.09195226194127534\n",
            "Iteration 100, Loss: 0.08651788326054576\n",
            "Iteration 101, Loss: 0.08140467635984766\n",
            "Iteration 102, Loss: 0.07659365998698062\n",
            "Iteration 103, Loss: 0.07206697468175022\n",
            "Iteration 104, Loss: 0.06780781647805834\n",
            "Iteration 105, Loss: 0.06380037452420508\n",
            "Iteration 106, Loss: 0.06002977238982451\n",
            "Iteration 107, Loss: 0.056482012841585764\n",
            "Iteration 108, Loss: 0.05314392588264784\n",
            "Iteration 109, Loss: 0.050003119862983315\n",
            "Iteration 110, Loss: 0.04704793547908108\n",
            "Iteration 111, Loss: 0.044267402492267266\n",
            "Iteration 112, Loss: 0.04165119900497404\n",
            "Iteration 113, Loss: 0.03918961314378035\n",
            "Iteration 114, Loss: 0.036873507006982977\n",
            "Iteration 115, Loss: 0.034694282742870286\n",
            "Iteration 116, Loss: 0.032643850632766785\n",
            "Iteration 117, Loss: 0.030714599060370322\n",
            "Iteration 118, Loss: 0.028899366255902493\n",
            "Iteration 119, Loss: 0.027191413710178584\n",
            "Iteration 120, Loss: 0.025584401159906914\n",
            "Iteration 121, Loss: 0.024072363051356495\n",
            "Iteration 122, Loss: 0.02264968639502138\n",
            "Iteration 123, Loss: 0.021311089929075627\n",
            "Iteration 124, Loss: 0.02005160451426725\n",
            "Iteration 125, Loss: 0.018866554687474075\n",
            "Iteration 126, Loss: 0.017751541305444478\n",
            "Iteration 127, Loss: 0.016702425214292563\n",
            "Iteration 128, Loss: 0.015715311884128023\n",
            "Iteration 129, Loss: 0.014786536951776086\n",
            "Iteration 130, Loss: 0.013912652617925996\n",
            "Iteration 131, Loss: 0.013090414848206543\n",
            "Iteration 132, Loss: 0.012316771330677616\n",
            "Iteration 133, Loss: 0.011588850145034585\n",
            "Iteration 134, Loss: 0.010903949101463065\n",
            "Iteration 135, Loss: 0.010259525709566468\n",
            "Iteration 136, Loss: 0.00965318774013127\n",
            "Iteration 137, Loss: 0.009082684344689475\n",
            "Iteration 138, Loss: 0.008545897699918217\n",
            "Iteration 139, Loss: 0.008040835145853157\n",
            "Iteration 140, Loss: 0.007565621788733219\n",
            "Iteration 141, Loss: 0.0071184935410191314\n",
            "Iteration 142, Loss: 0.0066977905727448606\n",
            "Iteration 143, Loss: 0.0063019511498957235\n",
            "Iteration 144, Loss: 0.0059295058369368625\n",
            "Iteration 145, Loss: 0.005579072041973911\n",
            "Iteration 146, Loss: 0.005249348884293189\n",
            "Iteration 147, Loss: 0.004939112365231465\n",
            "Iteration 148, Loss: 0.004647210824446307\n",
            "Iteration 149, Loss: 0.004372560664721486\n",
            "Iteration 150, Loss: 0.004114142329436494\n",
            "Iteration 151, Loss: 0.003870996517766834\n",
            "Iteration 152, Loss: 0.0036422206235667827\n",
            "Iteration 153, Loss: 0.003426965384714017\n",
            "Iteration 154, Loss: 0.0032244317304774505\n",
            "Iteration 155, Loss: 0.0030338678152062068\n",
            "Iteration 156, Loss: 0.0028545662273275238\n",
            "Iteration 157, Loss: 0.002685861363292443\n",
            "Iteration 158, Loss: 0.002527126956721865\n",
            "Iteration 159, Loss: 0.0023777737535795864\n",
            "Iteration 160, Loss: 0.00223724732474303\n",
            "Iteration 161, Loss: 0.0021050260078507234\n",
            "Iteration 162, Loss: 0.0019806189707867374\n",
            "Iteration 163, Loss: 0.0018635643896132343\n",
            "Iteration 164, Loss: 0.0017534277341871227\n",
            "Iteration 165, Loss: 0.001649800155096641\n",
            "Iteration 166, Loss: 0.0015522969659304577\n",
            "Iteration 167, Loss: 0.001460556215243966\n",
            "Iteration 168, Loss: 0.0013742373429230384\n",
            "Iteration 169, Loss: 0.0012930199159562866\n",
            "Iteration 170, Loss: 0.0012166024389232565\n",
            "Iteration 171, Loss: 0.0011447012347829176\n",
            "Iteration 172, Loss: 0.0010770493918072417\n",
            "Iteration 173, Loss: 0.0010133957727514104\n",
            "Iteration 174, Loss: 0.0009535040825818078\n",
            "Iteration 175, Loss: 0.0008971519913012032\n",
            "Iteration 176, Loss: 0.0008441303086153036\n",
            "Iteration 177, Loss: 0.0007942422073761319\n",
            "Iteration 178, Loss: 0.0007473024929201971\n",
            "Iteration 179, Loss: 0.0007031369155886336\n",
            "Iteration 180, Loss: 0.0006615815238773228\n",
            "Iteration 181, Loss: 0.0006224820558161947\n",
            "Iteration 182, Loss: 0.0005856933663174669\n",
            "Iteration 183, Loss: 0.0005510788883681015\n",
            "Iteration 184, Loss: 0.0005185101260655451\n",
            "Iteration 185, Loss: 0.00048786617761505856\n",
            "Iteration 186, Loss: 0.00045903328651801555\n",
            "Iteration 187, Loss: 0.0004319044192847635\n",
            "Iteration 188, Loss: 0.0004063788681050637\n",
            "Iteration 189, Loss: 0.0003823618770000461\n",
            "Iteration 190, Loss: 0.0003597642900693548\n",
            "Iteration 191, Loss: 0.0003385022205262612\n",
            "Iteration 192, Loss: 0.00031849673929316324\n",
            "Iteration 193, Loss: 0.0002996735820009465\n",
            "Iteration 194, Loss: 0.0002819628733046985\n",
            "Iteration 195, Loss: 0.0002652988674923804\n",
            "Iteration 196, Loss: 0.0002496197044235683\n",
            "Iteration 197, Loss: 0.00023486717989212869\n",
            "Iteration 198, Loss: 0.00022098652956051694\n",
            "Iteration 199, Loss: 0.0002079262256634926\n",
            "Iteration 200, Loss: 0.00019563778572677352\n",
            "Iteration 201, Loss: 0.0001840755925903419\n",
            "Iteration 202, Loss: 0.00017319672506825408\n",
            "Iteration 203, Loss: 0.00016296079861672692\n",
            "Iteration 204, Loss: 0.00015332981541847114\n",
            "Iteration 205, Loss: 0.00014426802332722987\n",
            "Iteration 206, Loss: 0.00013574178314859723\n",
            "Iteration 207, Loss: 0.00012771944376450736\n",
            "Iteration 208, Loss: 0.0001201712246380371\n",
            "Iteration 209, Loss: 0.00011306910526192702\n",
            "Iteration 210, Loss: 0.00010638672114094546\n",
            "Iteration 211, Loss: 0.00010009926592150059\n",
            "Iteration 212, Loss: 9.418339930554948e-05\n",
            "Iteration 213, Loss: 8.861716040658065e-05\n",
            "Iteration 214, Loss: 8.337988622655365e-05\n",
            "Iteration 215, Loss: 7.84521349505644e-05\n",
            "Iteration 216, Loss: 7.381561377498786e-05\n",
            "Iteration 217, Loss: 6.945311100088675e-05\n",
            "Iteration 218, Loss: 6.534843214072709e-05\n",
            "Iteration 219, Loss: 6.148633980121213e-05\n",
            "Iteration 220, Loss: 5.785249711895769e-05\n",
            "Iteration 221, Loss: 5.4433414539221796e-05\n",
            "Iteration 222, Loss: 5.121639973996355e-05\n",
            "Iteration 223, Loss: 4.81895105153235e-05\n",
            "Iteration 224, Loss: 4.534151044386368e-05\n",
            "Iteration 225, Loss: 4.266182717663222e-05\n",
            "Iteration 226, Loss: 4.014051319049326e-05\n",
            "Iteration 227, Loss: 3.776820886093738e-05\n",
            "Iteration 228, Loss: 3.5536107717259625e-05\n",
            "Iteration 229, Loss: 3.343592375117329e-05\n",
            "Iteration 230, Loss: 3.1459860657477434e-05\n",
            "Iteration 231, Loss: 2.960058289262165e-05\n",
            "Iteration 232, Loss: 2.785118844366614e-05\n",
            "Iteration 233, Loss: 2.620518320664197e-05\n",
            "Iteration 234, Loss: 2.465645687913609e-05\n",
            "Iteration 235, Loss: 2.3199260277578386e-05\n",
            "Iteration 236, Loss: 2.1828183995166534e-05\n",
            "Iteration 237, Loss: 2.0538138321052876e-05\n",
            "Iteration 238, Loss: 1.9324334346277887e-05\n",
            "Iteration 239, Loss: 1.8182266186417932e-05\n",
            "Iteration 240, Loss: 1.7107694254796536e-05\n",
            "Iteration 241, Loss: 1.60966295243339e-05\n",
            "Iteration 242, Loss: 1.5145318719448264e-05\n",
            "Iteration 243, Loss: 1.425023038312852e-05\n",
            "Iteration 244, Loss: 1.3408041767486096e-05\n",
            "Iteration 245, Loss: 1.26156264990324e-05\n",
            "Iteration 246, Loss: 1.1870042972937642e-05\n",
            "Iteration 247, Loss: 1.1168523433236544e-05\n",
            "Iteration 248, Loss: 1.05084636983339e-05\n",
            "Iteration 249, Loss: 9.887413493762888e-06\n",
            "Iteration 250, Loss: 9.303067356281333e-06\n",
            "Iteration 251, Loss: 8.753256075524225e-06\n",
            "Iteration 252, Loss: 8.235938641459775e-06\n",
            "Iteration 253, Loss: 7.749194667752531e-06\n",
            "Iteration 254, Loss: 7.291217262887631e-06\n",
            "Iteration 255, Loss: 6.860306322650024e-06\n",
            "Iteration 256, Loss: 6.454862218981791e-06\n",
            "Iteration 257, Loss: 6.073379861837828e-06\n",
            "Iteration 258, Loss: 5.714443112002188e-06\n",
            "Iteration 259, Loss: 5.3767195240850106e-06\n",
            "Iteration 260, Loss: 5.058955400210703e-06\n",
            "Iteration 261, Loss: 4.759971136057005e-06\n",
            "Iteration 262, Loss: 4.478656841918348e-06\n",
            "Iteration 263, Loss: 4.213968222560664e-06\n",
            "Iteration 264, Loss: 3.964922700606197e-06\n",
            "Iteration 265, Loss: 3.7305957690002805e-06\n",
            "Iteration 266, Loss: 3.510117559052027e-06\n",
            "Iteration 267, Loss: 3.3026696113123427e-06\n",
            "Iteration 268, Loss: 3.1074818372857597e-06\n",
            "Iteration 269, Loss: 2.923829660701465e-06\n",
            "Iteration 270, Loss: 2.7510313277554854e-06\n",
            "Iteration 271, Loss: 2.588445376284904e-06\n",
            "Iteration 272, Loss: 2.43546825454543e-06\n",
            "Iteration 273, Loss: 2.291532080702501e-06\n",
            "Iteration 274, Loss: 2.1561025347325692e-06\n",
            "Iteration 275, Loss: 2.0286768749291184e-06\n",
            "Iteration 276, Loss: 1.9087820716201727e-06\n",
            "Iteration 277, Loss: 1.7959730511882535e-06\n",
            "Iteration 278, Loss: 1.689831043863631e-06\n",
            "Iteration 279, Loss: 1.589962029172178e-06\n",
            "Iteration 280, Loss: 1.495995273247771e-06\n",
            "Iteration 281, Loss: 1.407581952598609e-06\n",
            "Iteration 282, Loss: 1.3243938592002331e-06\n",
            "Iteration 283, Loss: 1.2461221821205525e-06\n",
            "Iteration 284, Loss: 1.1724763611572662e-06\n",
            "Iteration 285, Loss: 1.1031830082130771e-06\n",
            "Iteration 286, Loss: 1.0379848924269536e-06\n",
            "Iteration 287, Loss: 9.766399852850823e-07\n",
            "Iteration 288, Loss: 9.189205621541337e-07\n",
            "Iteration 289, Loss: 8.64612356930779e-07\n",
            "Iteration 290, Loss: 8.13513766636162e-07\n",
            "Iteration 291, Loss: 7.654351030289691e-07\n",
            "Iteration 292, Loss: 7.201978884396668e-07\n",
            "Iteration 293, Loss: 6.776341932328168e-07\n",
            "Iteration 294, Loss: 6.375860124131066e-07\n",
            "Iteration 295, Loss: 5.99904679078761e-07\n",
            "Iteration 296, Loss: 5.644503125445073e-07\n",
            "Iteration 297, Loss: 5.310912990733696e-07\n",
            "Iteration 298, Loss: 4.99703803298259e-07\n",
            "Iteration 299, Loss: 4.701713085231401e-07\n",
            "Iteration 300, Loss: 4.4238418418969425e-07\n",
            "Iteration 301, Loss: 4.1623927890298885e-07\n",
            "Iteration 302, Loss: 3.9163953752014874e-07\n",
            "Iteration 303, Loss: 3.6849364085283465e-07\n",
            "Iteration 304, Loss: 3.4671566667892377e-07\n",
            "Iteration 305, Loss: 3.262247707780662e-07\n",
            "Iteration 306, Loss: 3.0694488682458796e-07\n",
            "Iteration 307, Loss: 2.8880444401372973e-07\n",
            "Iteration 308, Loss: 2.717361013719963e-07\n",
            "Iteration 309, Loss: 2.556764977809562e-07\n",
            "Iteration 310, Loss: 2.4056601676180983e-07\n",
            "Iteration 311, Loss: 2.263485651712534e-07\n",
            "Iteration 312, Loss: 2.1297136496970408e-07\n",
            "Iteration 313, Loss: 2.003847573001357e-07\n",
            "Iteration 314, Loss: 1.8854201814396186e-07\n",
            "Iteration 315, Loss: 1.7739918487140681e-07\n",
            "Iteration 316, Loss: 1.6691489304540961e-07\n",
            "Iteration 317, Loss: 1.5705022286648925e-07\n",
            "Iteration 318, Loss: 1.4776855469531447e-07\n",
            "Iteration 319, Loss: 1.3903543311229646e-07\n",
            "Iteration 320, Loss: 1.3081843901569785e-07\n",
            "Iteration 321, Loss: 1.2308706926978675e-07\n",
            "Iteration 322, Loss: 1.1581262347582523e-07\n",
            "Iteration 323, Loss: 1.0896809742855348e-07\n",
            "Iteration 324, Loss: 1.025280828706568e-07\n",
            "Iteration 325, Loss: 9.646867317281201e-08\n",
            "Iteration 326, Loss: 9.076737458845068e-08\n",
            "Iteration 327, Loss: 8.540302275002342e-08\n",
            "Iteration 328, Loss: 8.035570410580293e-08\n",
            "Iteration 329, Loss: 7.560668199290088e-08\n",
            "Iteration 330, Loss: 7.113832708718676e-08\n",
            "Iteration 331, Loss: 6.693405195627888e-08\n",
            "Iteration 332, Loss: 6.297824948567227e-08\n",
            "Iteration 333, Loss: 5.925623494117606e-08\n",
            "Iteration 334, Loss: 5.575419145621442e-08\n",
            "Iteration 335, Loss: 5.245911874146747e-08\n",
            "Iteration 336, Loss: 4.935878482375844e-08\n",
            "Iteration 337, Loss: 4.644168064079394e-08\n",
            "Iteration 338, Loss: 4.369697731487614e-08\n",
            "Iteration 339, Loss: 4.1114485955542195e-08\n",
            "Iteration 340, Loss: 3.868461983548362e-08\n",
            "Iteration 341, Loss: 3.639835880319764e-08\n",
            "Iteration 342, Loss: 3.4247215797814425e-08\n",
            "Iteration 343, Loss: 3.222320534423574e-08\n",
            "Iteration 344, Loss: 3.031881390831524e-08\n",
            "Iteration 345, Loss: 2.8526972006412187e-08\n",
            "Iteration 346, Loss: 2.684102796088525e-08\n",
            "Iteration 347, Loss: 2.525472320833024e-08\n",
            "Iteration 348, Loss: 2.3762169066755916e-08\n",
            "Iteration 349, Loss: 2.2357824874938528e-08\n",
            "Iteration 350, Loss: 2.1036477424740777e-08\n",
            "Iteration 351, Loss: 1.9793221608971083e-08\n",
            "Iteration 352, Loss: 1.8623442211890592e-08\n",
            "Iteration 353, Loss: 1.7522796777177556e-08\n",
            "Iteration 354, Loss: 1.6487199487584492e-08\n",
            "Iteration 355, Loss: 1.551280599784087e-08\n",
            "Iteration 356, Loss: 1.4595999163459952e-08\n",
            "Iteration 357, Loss: 1.373337561276728e-08\n",
            "Iteration 358, Loss: 1.292173311408378e-08\n",
            "Iteration 359, Loss: 1.2158058687015967e-08\n",
            "Iteration 360, Loss: 1.1439517418636596e-08\n",
            "Iteration 361, Loss: 1.0763441939095886e-08\n",
            "Iteration 362, Loss: 1.0127322520584254e-08\n",
            "Iteration 363, Loss: 9.52879775954663e-09\n",
            "Iteration 364, Loss: 8.965645811974876e-09\n",
            "Iteration 365, Loss: 8.435776144445761e-09\n",
            "Iteration 366, Loss: 7.937221774405157e-09\n",
            "Iteration 367, Loss: 7.468131967540116e-09\n",
            "Iteration 368, Loss: 7.026765368245838e-09\n",
            "Iteration 369, Loss: 6.61148353507531e-09\n",
            "Iteration 370, Loss: 6.220744858138874e-09\n",
            "Iteration 371, Loss: 5.853098837031191e-09\n",
            "Iteration 372, Loss: 5.507180695766107e-09\n",
            "Iteration 373, Loss: 5.1817063166587975e-09\n",
            "Iteration 374, Loss: 4.875467473291704e-09\n",
            "Iteration 375, Loss: 4.5873273456950585e-09\n",
            "Iteration 376, Loss: 4.316216299518383e-09\n",
            "Iteration 377, Loss: 4.0611279162175535e-09\n",
            "Iteration 378, Loss: 3.821115256353037e-09\n",
            "Iteration 379, Loss: 3.5952873447260055e-09\n",
            "Iteration 380, Loss: 3.382805862653215e-09\n",
            "Iteration 381, Loss: 3.18288203613571e-09\n",
            "Iteration 382, Loss: 2.99477370783946e-09\n",
            "Iteration 383, Loss: 2.8177825817423328e-09\n",
            "Iteration 384, Loss: 2.651251631128891e-09\n",
            "Iteration 385, Loss: 2.4945626597086566e-09\n",
            "Iteration 386, Loss: 2.3471340065450475e-09\n",
            "Iteration 387, Loss: 2.208418386724218e-09\n",
            "Iteration 388, Loss: 2.077900860093716e-09\n",
            "Iteration 389, Loss: 1.9550969192345886e-09\n",
            "Iteration 390, Loss: 1.8395506913247764e-09\n",
            "Iteration 391, Loss: 1.730833245463787e-09\n",
            "Iteration 392, Loss: 1.628541000614583e-09\n",
            "Iteration 393, Loss: 1.5322942274938195e-09\n",
            "Iteration 394, Loss: 1.4417356386098988e-09\n",
            "Iteration 395, Loss: 1.3565290623932424e-09\n",
            "Iteration 396, Loss: 1.2763581948115135e-09\n",
            "Iteration 397, Loss: 1.2009254255142351e-09\n",
            "Iteration 398, Loss: 1.1299507328377568e-09\n",
            "Iteration 399, Loss: 1.0631706445221947e-09\n",
            "Iteration 400, Loss: 1.000337259443855e-09\n",
            "Iteration 401, Loss: 9.412173273918535e-10\n",
            "Iteration 402, Loss: 8.855913833747784e-10\n",
            "Iteration 403, Loss: 8.332529326230976e-10\n",
            "Iteration 404, Loss: 7.840076842851773e-10\n",
            "Iteration 405, Loss: 7.376728301364451e-10\n",
            "Iteration 406, Loss: 6.940763658878999e-10\n",
            "Iteration 407, Loss: 6.530564526674998e-10\n",
            "Iteration 408, Loss: 6.144608163357662e-10\n",
            "Iteration 409, Loss: 5.78146182090536e-10\n",
            "Iteration 410, Loss: 5.439777427500114e-10\n",
            "Iteration 411, Loss: 5.118286581301266e-10\n",
            "Iteration 412, Loss: 4.815795844352208e-10\n",
            "Iteration 413, Loss: 4.5311823101362744e-10\n",
            "Iteration 414, Loss: 4.263389435406866e-10\n",
            "Iteration 415, Loss: 4.0114231201607844e-10\n",
            "Iteration 416, Loss: 3.7743480135133947e-10\n",
            "Iteration 417, Loss: 3.5512840460557673e-10\n",
            "Iteration 418, Loss: 3.341403158830776e-10\n",
            "Iteration 419, Loss: 3.1439262322730143e-10\n",
            "Iteration 420, Loss: 2.958120191949498e-10\n",
            "Iteration 421, Loss: 2.7832952886345475e-10\n",
            "Iteration 422, Loss: 2.6188025370863073e-10\n",
            "Iteration 423, Loss: 2.464031307258482e-10\n",
            "Iteration 424, Loss: 2.3184070569149824e-10\n",
            "Iteration 425, Loss: 2.1813891998450758e-10\n",
            "Iteration 426, Loss: 2.052469098200717e-10\n",
            "Iteration 427, Loss: 1.9311681744652723e-10\n",
            "Iteration 428, Loss: 1.817036135343899e-10\n",
            "Iteration 429, Loss: 1.7096492997819465e-10\n",
            "Iteration 430, Loss: 1.60860902612428e-10\n",
            "Iteration 431, Loss: 1.5135402326199638e-10\n",
            "Iteration 432, Loss: 1.424090004846951e-10\n",
            "Iteration 433, Loss: 1.3399262856327211e-10\n",
            "Iteration 434, Loss: 1.2607366421872305e-10\n",
            "Iteration 435, Loss: 1.186227106634207e-10\n",
            "Iteration 436, Loss: 1.1161210845992837e-10\n",
            "Iteration 437, Loss: 1.0501583285258613e-10\n",
            "Iteration 438, Loss: 9.880939713843652e-11\n",
            "Iteration 439, Loss: 9.296976175762081e-11\n",
            "Iteration 440, Loss: 8.747524883321811e-11\n",
            "Iteration 441, Loss: 8.230546163202973e-11\n",
            "Iteration 442, Loss: 7.744120884273774e-11\n",
            "Iteration 443, Loss: 7.286443339778167e-11\n",
            "Iteration 444, Loss: 6.855814538735565e-11\n",
            "Iteration 445, Loss: 6.450635899720999e-11\n",
            "Iteration 446, Loss: 6.069403317717082e-11\n",
            "Iteration 447, Loss: 5.7107015817322913e-11\n",
            "Iteration 448, Loss: 5.3731991181607656e-11\n",
            "Iteration 449, Loss: 5.055643049734355e-11\n",
            "Iteration 450, Loss: 4.756854545905481e-11\n",
            "Iteration 451, Loss: 4.475724441213017e-11\n",
            "Iteration 452, Loss: 4.21120912663214e-11\n",
            "Iteration 453, Loss: 3.9623266676926504e-11\n",
            "Iteration 454, Loss: 3.7281531616876014e-11\n",
            "Iteration 455, Loss: 3.5078193097029844e-11\n",
            "Iteration 456, Loss: 3.300507188194658e-11\n",
            "Iteration 457, Loss: 3.105447214003417e-11\n",
            "Iteration 458, Loss: 2.921915283707426e-11\n",
            "Iteration 459, Loss: 2.7492300909432725e-11\n",
            "Iteration 460, Loss: 2.5867505927322766e-11\n",
            "Iteration 461, Loss: 2.433873632862829e-11\n",
            "Iteration 462, Loss: 2.2900317009523704e-11\n",
            "Iteration 463, Loss: 2.1546908271941776e-11\n",
            "Iteration 464, Loss: 2.0273485995089572e-11\n",
            "Iteration 465, Loss: 1.9075322974244156e-11\n",
            "Iteration 466, Loss: 1.7947971384020527e-11\n",
            "Iteration 467, Loss: 1.688724627825432e-11\n",
            "Iteration 468, Loss: 1.588921002347502e-11\n",
            "Iteration 469, Loss: 1.4950157708769573e-11\n",
            "Iteration 470, Loss: 1.4066603390263264e-11\n",
            "Iteration 471, Loss: 1.3235267132798725e-11\n",
            "Iteration 472, Loss: 1.2453062841591044e-11\n",
            "Iteration 473, Loss: 1.171708682369308e-11\n",
            "Iteration 474, Loss: 1.1024606995251279e-11\n",
            "Iteration 475, Loss: 1.037305272279022e-11\n",
            "Iteration 476, Loss: 9.760005307372776e-12\n",
            "Iteration 477, Loss: 9.183188993202385e-12\n",
            "Iteration 478, Loss: 8.640462524565678e-12\n",
            "Iteration 479, Loss: 8.12981118631858e-12\n",
            "Iteration 480, Loss: 7.649339346251152e-12\n",
            "Iteration 481, Loss: 7.197263390953236e-12\n",
            "Iteration 482, Loss: 6.771905124154979e-12\n",
            "Iteration 483, Loss: 6.37168553237114e-12\n",
            "Iteration 484, Loss: 5.9951189198491045e-12\n",
            "Iteration 485, Loss: 5.640807392793488e-12\n",
            "Iteration 486, Loss: 5.307435675214386e-12\n",
            "Iteration 487, Loss: 4.993766225310701e-12\n",
            "Iteration 488, Loss: 4.6986346424537226e-12\n",
            "Iteration 489, Loss: 4.420945334939978e-12\n",
            "Iteration 490, Loss: 4.1596674666277435e-12\n",
            "Iteration 491, Loss: 3.913831120224211e-12\n",
            "Iteration 492, Loss: 3.6825236999025745e-12\n",
            "Iteration 493, Loss: 3.464886548808481e-12\n",
            "Iteration 494, Loss: 3.2601117537257896e-12\n",
            "Iteration 495, Loss: 3.0674391488783718e-12\n",
            "Iteration 496, Loss: 2.886153496345284e-12\n",
            "Iteration 497, Loss: 2.7155818244258695e-12\n",
            "Iteration 498, Loss: 2.55509093911695e-12\n",
            "Iteration 499, Loss: 2.404085064260527e-12\n",
            "Iteration 500, Loss: 2.2620036369560507e-12\n",
            "Final weights: [-3.3999999  -0.20000019  0.80000029]\n",
            "Final bias: 0.6000000972584016\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Initial parameters\n",
        "weights = np.array([-3.0, -1.0, 2.0])\n",
        "bias = 1.0\n",
        "inputs = np.array([1.0, -2.0, 3.0])\n",
        "target_output = 0.0\n",
        "learning_rate = 0.001\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "# when the iteration increases after 211, the loss starts to increase - need to understand why\n",
        "for iteration in range(200):\n",
        "  weighted_sum = np.dot(inputs, weights) + bias\n",
        "  relu_output = relu(weighted_sum)\n",
        "  loss = (relu_output - target_output) **2\n",
        "\n",
        "  dloss_drelu_output = 2 * (relu_output-target_output)\n",
        "  drelu_output_dweighted_sum = relu_derivative(weighted_sum)\n",
        "  dweighted_sum_dbias = 1.0\n",
        "  dweighted_sum_dweights = inputs\n",
        "\n",
        "  dloss_dweights = dloss_drelu_output * drelu_output_dweighted_sum * dweighted_sum_dweights\n",
        "  dloss_dbias = dloss_drelu_output * drelu_output_dweighted_sum * dweighted_sum_dbias\n",
        "\n",
        "  weights -= learning_rate * dloss_dweights\n",
        "  bias -= learning_rate * dloss_dbias\n",
        " # Print the loss for this iteration\n",
        "  print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n",
        "\n",
        "print(\"Final weights:\", weights)\n",
        "print(\"Final bias:\", bias)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}